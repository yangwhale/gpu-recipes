# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

{{ $nodes := div .Values.job.gpus 8 | max 1 }}
{{ $gpusPerNode := min .Values.job.gpus 8 }}

{{- $root := . -}}

apiVersion: apps/v1
kind: Deployment

metadata:
  name: {{ .Release.Name }}-serving
  labels:
    app: {{ .Release.Name }}-serving
spec:
  replicas: {{ $nodes }}
  selector:
    matchLabels:
      app: {{ .Release.Name }}-serving
  template:
    metadata:
      labels:
        app: {{ .Release.Name }}-serving
      annotations:
        kubectl.kubernetes.io/default-container: serving

        {{- if .Values.volumes.gcsMounts }}
        gke-gcsfuse/volumes: "true"
        gke-gcsfuse/cpu-limit: "0"
        gke-gcsfuse/memory-limit: "0"
        gke-gcsfuse/ephemeral-storage-limit: "0"
        {{- end}}
        {{- if not $root.Values.network.hostNetwork }}
        networking.gke.io/default-interface: "eth0"
        networking.gke.io/interfaces: |
        {{- if $root.Values.network.subnetworks }}
          [
            {{- range $i, $subnetwork := $root.Values.network.subnetworks }}
            {"interfaceName":"eth{{ $i }}","network":"{{ $subnetwork }}"}{{ eq $i 9 | ternary "" ","}}
            {{- end }}
          ]
        {{- else }}
          [
            {"interfaceName":"eth0","network":"default"},
            {"interfaceName":"eth1","network":"gvnic-1"},
            {{- range  $i := until 8 }}
            {"interfaceName":"eth{{ add 2 $i }}","network":"rdma-{{ $i }}"}{{ eq $i 7 | ternary "" ","}}
            {{- end }}
          ]
        {{- end }}
        {{- end }}

    spec:
      {{- if $root.Values.network.hostNetwork }}
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      {{- end }}
      restartPolicy: Always

      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
      - key: cloud.google.com/impending-node-termination
        operator: Exists
      volumes:
        {{ if $root.Values.network.gibVersion }}
        - name: gib
          emptyDir: {}
        {{ end }}
        - name: library-dir-host
          hostPath:
            path: /home/kubernetes/bin/nvidia
        - name: shared-memory
          emptyDir:
            medium: "Memory"
            sizeLimit: 250Gi
        - name: local-ssd
          hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd
        {{- range $gcs := .Values.volumes.gcsMounts }}
        - name: "{{ $gcs.bucketName }}"
          csi:
            driver: gcsfuse.csi.storage.gke.io
            volumeAttributes:
              bucketName: "{{ $gcs.bucketName }}"
        {{- end }}

      initContainers:
      {{ if $root.Values.network.gibVersion }}
      - name: nccl-plugin-installer
        image: {{ $root.Values.network.gibVersion }}
        imagePullPolicy: Always
        args:
        - |
          set -ex
          /scripts/container_entry.sh install --install-nccl
          cp -R /var/lib/gib/lib64/. /target/usr/local/gib/lib64
          cp -R /var/lib/gib/. /target/usr/local/gib
        command:
        - /bin/sh
        - -c

        volumeMounts:
        - mountPath: /target/usr/local/gib
          name: gib

      {{ end }}

      containers:
      - name: serving
        image: "{{ .Values.job.image.repository }}:{{ .Values.job.image.tag }}"
        imagePullPolicy: Always
        securityContext:
          privileged: true
        resources:
          requests:
            nvidia.com/gpu: {{ $gpusPerNode }}
          limits:
            nvidia.com/gpu: {{ $gpusPerNode }}

        env:
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: "{{ .Values.huggingface.secretName }}"
                key: "{{ .Values.huggingface.secretData.token }}"

          # Additional NCCL settings
          {{- range $environment_variable := $root.Values.network.ncclSettings }}
          - name: {{ $environment_variable.name }}
            value: "{{ $environment_variable.value }}"
          {{- end }}

          # Enable faster downloads of model weights from HuggingFace
          - name: HF_HUB_ENABLE_HF_TRANSFER
            value: "1"
          - name: LD_LIBRARY_PATH
            value: "/usr/local/nvidia/lib64:/usr/local/lib/"
          # Workload specific environment variables
          - name: MODEL_DOWNLOAD_DIR
            value: "/ssd/{{ .Values.model.name }}"

          {{- range $gcs := $root.Values.volumes.gcsMounts }}
          - name: GCS_FUSE_BUCKET
            value: "{{ $gcs.bucketName }}"
          {{- end }}

        workingDir: /vllm-workspace
        command: ["/bin/bash", "-c"]
        args:
          - |
            #!/bin/bash

            # Set recommended NCCL environment variables
            apt-get update && apt-get install -y --no-install-recommends pciutils
            cat /usr/local/gib/scripts/set_nccl_env.sh
            source /usr/local/gib/scripts/set_nccl_env.sh
            ldconfig

            # Build and install vLLM nightly from source (main)
            # Workaround until a new version of vLLM is released with Blackwell GPU support.

            echo -e "Building and installing vLLM nightly from source. This can take ~10 minutes."
            pip uninstall -y vllm
            apt-get update && apt-get install -y --no-install-recommends \
              ccache \
              kmod \
              git \
              python3-pip \
              && apt-get clean && rm -rf /var/lib/apt/lists/*

            # Clone vLLM from source
            git clone -b v0.8.0 https://github.com/vllm-project/vllm.git /vllm-workspace/vllm && \
            cd /vllm-workspace/vllm

            # Build vLLM
            python3 /vllm-workspace/vllm/use_existing_torch.py && \
            pip install -r requirements/build.txt && \
            pip install setuptools_scm && \
            pip install --force-reinstall -v "numpy<2.0" && \
            MAX_JOBS=200 python3 setup.py develop

            # Copy vLLM to the dist-packages directory
            cp -r /vllm-workspace/vllm/vllm /usr/local/lib/python3.12/dist-packages/vllm
            vllm -v
            echo -e "vLLM build completed successfully."

            # Build completed

            # Launch the server
            VLLM_USE_V1=1 VLLM_FLASH_ATTN_VERSION=2 VLLM_WORKER_MULTIPROC_METHOD=spawn vllm serve \
            {{ .Values.model.name }} \
            --download_dir /ssd \
            --trust-remote-code \
            --tensor-parallel-size {{ .Values.model.tp_size }} \
            --disable-log-requests

        volumeMounts:
          {{ if $root.Values.network.gibVersion }}
          - name: gib
            mountPath: /usr/local/gib
          {{ end }}
          - name: library-dir-host
            mountPath: /usr/local/nvidia
          - name: shared-memory
            mountPath: /dev/shm
          - name: local-ssd
            mountPath: {{ .Values.volumes.ssdMountPath }}
          {{- range $gcs := .Values.volumes.gcsMounts }}
          - name: "{{ $gcs.bucketName }}"
            mountPath: "{{ $gcs.mountPath }}"
          {{- end }}
