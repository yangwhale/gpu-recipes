{{ $timestamp := now | unixEpoch }}
{{ $jobSuffix := randAlphaNum 4 | lower }}
{{ $jobuuid := uuidv4 }}

{{ $nodes := div .Values.workload.gpus 8 | max 1 }}
{{ $gpusPerNode := min .Values.workload.gpus 8 }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: "{{ .Release.Name }}" 
data:
  nemo-configuration.yaml: |-
{{ .Files.Get "selected-configuration.yaml" | nindent 4 }}
---
apiVersion: v1
kind: Service
metadata:
  name: "{{ .Release.Name }}"
spec:
  clusterIP: None
  selector:
    job-name: "{{ .Release.Name }}"
---
{{- $root := . -}}
apiVersion: batch/v1
kind: Job
metadata:
  name: "{{ .Release.Name }}"
  namespace: default
  labels:
  {{- if $root.Values.queue }}
    kueue.x-k8s.io/queue-name: "{{ $root.Values.queue }}"
  {{- end }}
spec:
  {{- if $root.Values.queue }}
  suspend: true
  {{- end }}
  parallelism: {{ $nodes }}
  completions: {{ $nodes }}
  completionMode: Indexed
  ttlSecondsAfterFinished: 43200
  template:
   metadata:
    annotations:
      kubectl.kubernetes.io/default-container: nemo   
      {{- if $root.Values.volumes.gcsMounts }}
      gke-gcsfuse/volumes: "true"
      {{- end}}
      kueue.x-k8s.io/podset-preferred-topology: "kubernetes.io/hostname"
      networking.gke.io/default-interface: 'eth0'
      networking.gke.io/interfaces: |
        [
          {"interfaceName":"eth0","network":"default"},
          {"interfaceName":"eth1","network":"gvnic-1"},
          {"interfaceName":"eth2","network":"rdma-0"},
          {"interfaceName":"eth3","network":"rdma-1"},
          {"interfaceName":"eth4","network":"rdma-2"},
          {"interfaceName":"eth5","network":"rdma-3"},
          {"interfaceName":"eth6","network":"rdma-4"},
          {"interfaceName":"eth7","network":"rdma-5"},
          {"interfaceName":"eth8","network":"rdma-6"},
          {"interfaceName":"eth9","network":"rdma-7"}
        ] 

   spec:
    # serviceAccountName: a3ultra-benchmark-sa
    # serviceAccountName: k8s-sa
    # serviceAccountName: map-a4-gke
    # schedulingGates:
    # - name: "gke.io/topology-aware-auto-scheduling"
    hostNetwork: true
    dnsPolicy: ClusterFirstWithHostNet
    subdomain: "{{.Release.Name}}"
    restartPolicy: Never

#    affinity:
#      nodeAffinity:
#        requiredDuringSchedulingIgnoredDuringExecution:
#          nodeSelectorTerms:
#          - matchExpressions:
#            - key: cloud.google.com/gce-topology-block
#              operator: In
#              values:
#              - 96ede179544f2de334cb74b6a8665c86
# 
    {{ if $root.Values.targetNodes }}
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              {{- range $hostname := $root.Values.targetNodes }}
              - {{ $hostname }}
              {{- end }} 
    {{ end }}
       
    tolerations:
    - operator: "Exists"
      key: nvidia.com/gpu
    - operator: "Exists"
      key: cloud.google.com/impending-node-termination 
    - operator: "Exists"
      key: sufkha-lease-36n-for-map

    volumes:
    {{ if eq $root.Values.targetPlatform "gke" }}
    - name: nvidia-install-dir-host
      hostPath:
        path: /home/kubernetes/bin/nvidia
    {{ else }}
    - name: dmabuf
      hostPath:
        path: /dev/dmabuf_import_helper
        type: CharDevice        
    - name: cuda-lib
      hostPath:
        path: /usr/lib/x86_64-linux-gnu/libcuda.so       
    - name: cuda-lib1
      hostPath:
        path: /usr/lib/x86_64-linux-gnu/libcuda.so.1
    - name: cuda-lib535
      hostPath:
        path: /usr/lib/x86_64-linux-gnu/libcuda.so.535.104.12
    {{ end }}
    - name: gib
      hostPath:
        path: /home/kubernetes/bin/gib
    - name: nccl-plugin-volume
      emptyDir: {}    
    {{ if ne $root.Values.network.stack "tcp" }}
    - name: tcpx-daemon-socket
      hostPath:
        path: /run/tcpx
    {{ end }} 
    - name: workload-configuration
      configMap:
        name: "{{.Release.Name}}"
    - name: workload-terminated-volume
      emptyDir: {}        
    - name: local-ssd
      hostPath:
        path: /mnt/stateful_partition/kube-ephemeral-ssd   
    - name: shared-memory
      emptyDir:
        medium: "Memory"
        sizeLimit: 250Gi

    {{- range $pvc := $root.Values.volumes.pvcMounts }}
    - name: "{{ $pvc.name }}"
      persistentVolumeClaim:
        claimName: "{{ $pvc.name }}"
    {{- end }}    

    {{- range $gcs := $root.Values.volumes.gcsMounts }}
    - name: "{{ $gcs.bucketName }}"
      csi:
        driver: gcsfuse.csi.storage.gke.io
        volumeAttributes:
          bucketName: "{{ $gcs.bucketName }}"
    {{- end}}

    initContainers:

    - name: training-data-downloader
      image: gcr.io/google.com/cloudsdktool/google-cloud-cli
      volumeMounts:
      - name: local-ssd
        mountPath: "{{ $root.Values.volumes.ssdMountPath }}"

      {{- range $pvc := $root.Values.volumes.pvcMounts }}
      - name: "{{ $pvc.name }}"
        mountPath: "{{ $pvc.mountPath }}"
      {{- end }}

      {{- range $gcs := $root.Values.volumes.gcsMounts }}
      - name: "{{ $gcs.bucketName }}"
        mountPath: "{{ $gcs.mountPath }}"
      {{- end }}

      env:
      - name: GCS_DATA_SOURCE
        value: "{{ $root.Values.gcsDownload.source }}"
      - name: GCS_DATA_TARGET
        value: "{{ $root.Values.gcsDownload.target }}"
      command:
        - /bin/sh
        - -c
        - |
          echo "Caching training data from $GCS_DATA_SOURCE to $GCS_DATA_TARGET"
          mkdir -p $GCS_DATA_TARGET

          SECONDS=0
          gcloud storage rsync \
            --recursive \
            $GCS_DATA_SOURCE $GCS_DATA_TARGET
          duration=$SECONDS
          echo "Transferred or synchronized $GCS_DATA_SOURCE to $GCS_DATA_TARGET in $duration seconds."
#   {{ if ne $root.Values.network.stack "tcp" }}
#   initContainers:
#   - name: nccl-plugin-installer
#     image: "{{ $root.Values.network.pluginVersion }}"
#     imagePullPolicy: Always
#     volumeMounts:
#     - name: nccl-plugin-volume
#       mountPath: /usr/local/nccl-plugin
#     command:
#       - /bin/sh
#       - -c
#       - |
#         mkdir -p /var/lib/gib
#         ln -s /var/lib/gib /var/lib/tcpx
#         ln -s /var/lib/gib /var/lib/tcpxo
#         /scripts/container_entry.sh install --install-nccl
#         cp -r /var/lib/gib/* /usr/local/nccl-plugin/
#         echo "Installed NCCL plugin to pod-wide, shared NCCL plug-in volume"
#         echo "Contents (mounted at /usr/local/nccl-plugin/lib64):"
#         ls /usr/local/nccl-plugin/lib64 | sed 's/^/  /'
#         echo "Contents (mounted at /usr/local/nccl-plugin/):"
#         ls /usr/local/nccl-plugin/ | sed 's/^/  /'

#   {{ end }}

    # Either the tcpx or tcpxo receive daemon
#   {{ if ne $root.Values.network.stack "gib" }}
#   - name: network-rx-daemon
#     image: "{{ $root.Values.network.daemonVersion }}"
#     imagePullPolicy: Always
#     restartPolicy: Always
#     securityContext:
#       privileged: true
#     volumeMounts:
#     - name: tcpx-daemon-socket
#       mountPath: /tmp
#     - name: workload-terminated-volume
#       mountPath: /semaphore
#     {{ if eq $root.Values.targetPlatform "gke" }}
#     - name: nvidia-install-dir-host
#       mountPath: "/usr/local/nvidia"
#     {{ else }}
#     - name: dmabuf
#       mountPath: /dev/dmabuf_import_helper
#     - name: cuda-lib
#       mountPath: /usr/lib/x86_64-linux-gnu/libcuda.so
#     - name: cuda-lib1
#       mountPath: /usr/lib/x86_64-linux-gnu/libcuda.so.1
#     - name: cuda-lib535
#       mountPath: /usr/lib/x86_64-linux-gnu/libcuda.so.535.104.12  
#     {{ end }}
#     env:
#     - name: LD_LIBRARY_PATH
#     {{ if eq $root.Values.targetPlatform "gke" }}
#       value: /usr/local/nvidia/lib64
#     {{ else }}
#       value: /usr/local/cuda-12.2/lib64 
#     {{ end }}

#     {{ if eq $root.Values.network.stack "tcpx" }}
#     command:
#     - bash
#     - -c
#     - |
#       /tcpgpudmarxd/build/app/tcpgpudmarxd --gpu_nic_preset a3vm --gpu_shmem_type fd --setup_param "--verbose 128 2 0"
#     {{ end }} 

#     {{ if eq $root.Values.network.stack "tcpxo" }}
#     command:
#     - bash
#     - -c
#     - |
#       /fts/entrypoint_rxdm_container.sh --num_hops 2 --num_nics 8 --uid=  --alsologtostderr
#     {{ end }}

#   {{ end }} 

    # schedulerName: topo-aware-scheduler
    containers:
 
    - name: nemo
      image: "{{ $root.Values.workload.image }}"
      imagePullPolicy: Always
      securityContext:
        privileged: true      
      env:
      - name: JOB_IDENTIFIER
        value: "{{ .Release.Name }}-{{ $timestamp }}-{{ $jobSuffix }}"
      - name: JOB_TIMESTAMP
        value: "{{ $timestamp }}"
      - name: JOB_UUID
        value: "{{ $jobuuid }}"
      - name: JOB_ORCHESTRATOR
        value: "gke"

      - name: SSD_MOUNT_PATH
        value: "{{ $root.Values.volumes.ssdMountPath }}"      

      {{- if $root.Values.volumes.jitGcsMount }}
      - name: JIT_GCS_FUSE_BUCKET
        value: "{{ $root.Values.volumes.jitGcsMount.bucketName }}"
      - name: JIT_GCS_FUSE_MOUNT_PATH
        value: "{{ $root.Values.volumes.jitGcsMount.mountPath }}"   
      {{ end }}

      # The following settings are specific to the Torch distributed launcher:
      - name: TORCH_DISTRIBUTED_TARGET
        value: "{{ $root.Values.workload.torchDistributedTarget }}"
      - name: TORCH_DISTRIBUTED_TRACING
        value: "ALL"

      - name: JOB_NAME
        value: "{{ .Release.Name }}"
      - name: MASTER_ADDR
        value: "{{.Release.Name}}-0.{{.Release.Name}}.default.svc.cluster.local"
      - name: MASTER_PORT
        value: "6002"
      - name: WORLD_SIZE
        value: "{{ $root.Values.workload.gpus }}"        
      - name: NNODES
        value: "{{ $nodes }}"        
      - name: GPUS_PER_NODE
        value: "{{ $gpusPerNode }}"        
      - name: GLOO_SOCKET_IFNAME
      {{ if eq $root.Values.targetPlatform "gke" }}
        value: "eth0"
      {{ else }}
        value: "enp0s12"
      {{ end }}   

      # The leader node can launch an embedded Tensorboard server (if needed)
      {{- if $root.Values.workload.embeddedTensorboardTarget }}
      - name: EMBEDDED_TENSORBOARD_TARGET
        value: "{{ $root.Values.workload.embeddedTensorboardTarget}}"             
      {{- end }}

      # The following arguments are passed to the Workload:
      {{- range $environment_variable := $root.Values.workload.arguments }}
      - name: "WORKLOAD_{{ $environment_variable.name }}"
        value: "{{ $environment_variable.value }}"
      {{- end }}       

      # The following is needed to prevent send-receive stalling execution
      - name: NVTE_FWD_LAYERNORM_SM_MARGIN
        value: "8"
      - name: NVTE_BWD_LAYERNORM_SM_MARGIN
        value: "8"    
         
      {{ if ne $root.Values.network.stack "tcp" }}      
 
      # The following GIB settings should likely not be adjusted:
      {{ if eq $root.Values.network.stack "gib" }}
      - name: NCCL_IB_GIB_INDEX
        value: "3"
      - name: NCCL_IB_QPS_PER_CONNECTION
        value: "8"
      {{ end }}
  
      # The following TCPxo settings should likely not be adjusted:
      {{ if eq $root.Values.network.stack "tcpxo" }}
      - name: NCCL_BUFFSIZE
        value: "8388608"   
      - name: NCCL_FASTRAK_CTRL_DEV
      {{ if eq $root.Values.targetPlatform "gke" }}
        value: "eth0"
      {{ else }}
        value: "enp0s12"
      {{ end }}
      - name: NCCL_FASTRAK_IFNAME
      {{ if eq $root.Values.targetPlatform "gke" }}
        value: "eth1,eth2,eth3,eth4,eth5,eth6,eth7,eth8"
      {{ else }}
        value: "enp6s0f0,enp7s0f0,enp13s0f0,enp14s0f0,enp134s0f0,enp135s0f0,enp141s0f0,enp142s0f0"
      {{ end }}
      - name: NCCL_FASTRAK_NUM_FLOWS
        value: "2"
      - name: NCCL_FASTRAK_NUM_FLOWS_PER_GROUP
        value: "1"        
      - name: NCCL_FASTRAK_ENABLE_CONTROL_CHANNEL
        value: "0"
      - name: NCCL_FASTRAK_ENABLE_HOTPATH_LOGGING
        value: "0"       
      - name: NCCL_FASTRAK_USE_SNAP
        value: "1"
      - name: NCCL_FASTRAK_USE_LLCM
        value: "1"                

      # The following NCCL tuner settings should likely not be adjusted: 
      - name: NCCL_TUNER_PLUGIN
        value: "libnccl-tuner.so"
      - name: NCCL_TUNER_CONFIG_PATH
        value: "/usr/local/nccl-plugin/lib64/a3plus_tuner_config_ll128.textproto"
      - name: NCCL_SHIMNET_GUEST_CONFIG_CHECKER_CONFIG_FILE
        value: "/usr/local/nccl-plugin/lib64/a3plus_guest_config_ll128.textproto"

      {{ end }}

      {{ if eq $root.Values.network.stack "tcpx" }}
      - name: NCCL_GPUDIRECTTCPX_CTRL_DEV
        value: "eth0"
      - name: NCCL_GPUDIRECTTCPX_SOCKET_IFNAME
        value: "eth1,eth2,eth3,eth4,eth5,eth6,eth7,eth8"
      - name: NCCL_GPUDIRECTTCPX_TX_BINDINGS
        value: "eth1:8-21,112-125;eth2:8-21,112-125;eth3:60-73,164-177;eth4:60-73,164-177"
      - name: NCCL_GPUDIRECTTCPX_RX_BINDINGS
        value: "eth1:22-35,126-139;eth2:22-35,126-139;eth3:74-87,178-191;eth4:74-87,178-191"        
      - name: NCCL_GPUDIRECTTCPX_PROGRAM_FLOW_STEERING_WAIT_MICROS
        value: "500000"
      {{ end }}
          
       # The following NCCL settings should likely not be adjusted:
      - name: NCCL_SOCKET_IFNAME
      {{ if eq $root.Values.targetPlatform "gke" }}
        value: "eth0"
      {{ else }}
        value: "enp0s12"
      {{ end }}
      
      - name: NCCL_P2P_NET_CHUNKSIZE
        value: "524288"
      - name: NCCL_P2P_PCI_CHUNKSIZE
        value: "524288"
      - name: NCCL_P2P_NVL_CHUNKSIZE
        value: "1048576"
      - name: NCCL_CROSS_NIC
        value: "0"
      - name: NCCL_NET_GDR_LEVEL
        value: "PIX"

      {{ if eq $root.Values.network.stack "gib" }}
      - name: NCCL_P2P_PXN_LEVEL
        value: "2"     
      - name: NCCL_NVLS_CHUNKSIZE 
        value: "524288"
      {{ else }}
      - name: NCCL_DYNAMIC_CHUNK_SIZE
        value: "524288"
      - name: NCCL_PROTO
        value: "Simple,LL128"
      - name: NCCL_P2P_PXN_LEVEL
        value: "0"
      - name: NCCL_NVLS_ENABLE
        value: "0"
      {{ end }}

      {{- range $environment_variable := $root.Values.network.ncclSettings }}
      - name: {{ $environment_variable.name }}
        value: "{{ $environment_variable.value }}"
      {{- end }}
 
      {{ end }}

      command:
      - bash
      - -c
      - |
        function on_script_completion {
          # Note: This semaphore is used to terminate the TCPx side-car
          touch /semaphore/workload_terminated
        }
        trap on_script_completion EXIT
        trap "" SIGPROF

        echo "Pod on $(hostname --fqdn) is running"
        echo "Pod is assigned job index of $JOB_COMPLETION_INDEX"
        echo "Job ID is $JOB_IDENTIFIER"

        mkdir /run/sshd
        /usr/sbin/sshd -p 2222
        echo "Pod has started SSH daemon"
 
        echo "The following GPUs are visible via nvidia-smi:"
        nvidia-smi --list-gpus

        # Note: Including this prevented some past errors. It might become deprecated in future.
        ldconfig /usr/local/nvidia/lib64/
        echo "Added /usr/local/nvidia/lib64/ to ldconfig. Note:"
        ldconfig -p | grep libcuda | sed 's/^/  /'

        # Note: Including this prevented some past errors. It might become deprecated in future.
        mount /tmp -o remount,exec 
        chmod -R a+rwx /tmp

        touch $SSD_MOUNT_PATH/hello-from-$HOSTNAME.txt
        echo "Local SSD contents (path $SSD_MOUNT_PATH):"; ls $SSD_MOUNT_PATH | sed 's/^/  /'

        # echo "GCP NCCL bundle contents are (path /usr/local/nccl-plugin/lib64):"
        # ls /usr/local/nccl-plugin/lib64 | sed 's/^/  /'

        echo "Contents (mounted at /usr/local/gib/):"
        ls /usr/local/gib

        echo "Contents (mounted at /usr/local/gib/lib64):"
        ls /usr/local/gib/lib64

        echo "Contents (mounted at /usr/local/gib/configs):"
        ls /usr/local/gib/configs

        echo "Setting NCCL environment variables"
        cat /usr/local/gib/scripts/set_nccl_env.sh
        source /usr/local/gib/scripts/set_nccl_env.sh

        set "Overriding NCCL_SOCKET_IFNAME definition"
        export NCCL_SOCKET_IFNAME="eth0,eth1"
 
        # Note: This is an alternative LD_LIBRARY_PATH used for custom k8s + Debian:
        # export LD_LIBRARY_PATH="/usr/local/nccl-plugin/lib64:/usr/local/cuda-12.3/lib64:/usr/local/nvidia/lib64/:${LD_LIBRARY_PATH}"

        export LD_LIBRARY_PATH="/third_party/nccl-netsupport/build/lib/:/usr/local/gib/lib64:/usr/local/nccl-plugin/lib64:/usr/local/nvidia/lib64/:${LD_LIBRARY_PATH}"
        echo "Setting LD_LIBRARY_PATH=$LD_LIBRARY_PATH in an attempt to override the built-in NCCL library used!"

        if ! [ -z ${JIT_GCS_FUSE_BUCKET} ]; then
          echo "Got request to JIT mount GCS bucket $JIT_GCS_FUSE_BUCKET via 'gcsfuse' to $JIT_GCS_FUSE_MOUNT_PATH:"
          mkdir -p $JIT_GCS_FUSE_MOUNT_PATH
          gcsfuse --client-protocol http2 $JIT_GCS_FUSE_BUCKET $JIT_GCS_FUSE_MOUNT_PATH 
        fi

        # It's for the GPT-2 tokenization scheme. To be removed in future.
        echo "Downloading GPT vocabulary files"
        wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json &&\
        wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt

        echo "NeMo configuration file:"                                         
        cat /etc/workload-configuration/nemo-configuration.yaml | sed 's/^/| /' 
        echo ""                                                                                                                                                
        readarray -d "" workload_arguments < <(env | grep -e "^WORKLOAD_" | sed 's/^WORKLOAD_/+/' | tr '\n' '\0') 
        echo "Detected the following additional workload arguments:"            
        for workload_argument in "${workload_arguments[@]}"; do                 
          echo "  $workload_argument"                                           
        done 
       
        sleep 10 # <- Hack to allow some time for service to boot

        echo "Checking for presence of nsys:"                                   
        which nsys  

        echo "NeMo job artifacts will go to /gcs/nemo-experiments/$JOB_IDENTIFIER/"
        mkdir -p /gcs/nemo-experiments/$JOB_IDENTIFIER/

        echo "Installing debugging tools"
        python -V
        apt -y update && apt -y install gdb python3.12-dbg
        pip install austin-dist austin-tui

        export NODE_RANK=$JOB_COMPLETION_INDEX                                  
        if [ "$NODE_RANK" -eq "0" ] && { ! [ -z ${EMBEDDED_TENSORBOARD_TARGET} ]; }; then
          echo "Launching an embedded Tensorboard against log directory $EMBEDDED_TENSORBOARD_TARGET"
          tensorboard --logdir $EMBEDDED_TENSORBOARD_TARGET &
        fi

        if [ "$JOB_COMPLETION_INDEX" -eq "0" ]; then
          echo "Delaying 10 sec to allow SSH service to start"
          sleep 10

          echo "List of worker services:"
          for JOB_INDEX in $(seq 0 $((NNODES-1))); do
            WORKER="$JOB_NAME-$JOB_INDEX.$JOB_NAME.default.svc.cluster.local"
  
            echo "  Ping $WORKER"
            echo -n "  Pong "; ssh -o LogLevel=ERROR -o StrictHostKeyChecking=no -p 2222 $WORKER hostname 
            ssh_exit_code=$?
            while [ $ssh_exit_code -ne 0 ]; do
              echo "  (pong failed, retrying in 2 seconds)"
              sleep 2
              echo -n "  Pong "; ssh -o LogLevel=ERROR -o StrictHostKeyChecking=no -p 2222 $WORKER hostname 
              ssh_exit_code=$?
            done
  
            echo "  Querying $WORKER for VM physical location"
            LOCATION=$(ssh -o LogLevel=ERROR -o StrictHostKeyChecking=no -p 2222 $WORKER curl "-s" "-H" "\"Metadata-Flavor: Google\"" "http://metadata.google.internal/computeMetadata/v1/instance/attributes/physical_host" )
  
            ssh_exit_code=$?
            while [ $ssh_exit_code -ne 0 ]; do
              echo "  (query failed, retrying in 2 seconds)"
              sleep 2
              LOCATION=$(ssh -o LogLevel=ERROR -o StrictHostKeyChecking=no -p 2222 $WORKER curl "-s" "-H" "\"Metadata-Flavor: Google\"" "http://metadata.google.internal/computeMetadata/v1/instance/attributes/physical_host" )
              ssh_exit_code=$?
            done
  
            echo "  Got $LOCATION"
            echo "$LOCATION $WORKER" >> "/tmp/${JOB_NAME}-job-worker-locations.txt"
  
            echo "Host $WORKER" >> /root/.ssh/config
            echo "  Port 2222" >> /root/.ssh/config
            echo "  StrictHostKeyChecking no" >> /root/.ssh/config
          done

          # Topology sort (not really needed with the latest topology scheduler)
          echo "Sorting VM list by physical location:"
          sort "/tmp/${JOB_NAME}-job-worker-locations.txt" > /tmp/job-worker-rank-order.txt
        
          # Ignore topology (the latest topology scheduler actually puts ranks in order)
          # echo "Random VM list by physical location:"
          # cat "/tmp/${JOB_NAME}-job-worker-locations.txt" > /tmp/job-worker-rank-order.txt

          cat /tmp/job-worker-rank-order.txt | sed 's/^/  /'
          for WORKER in $(cat /tmp/job-worker-rank-order.txt | awk '{print $2}'); do
            echo "$WORKER slots=8" >> /etc/job-worker-services.txt
          done
  
          # echo "Sleeping so you can interactively login"
          # sleep infinity
  
          readarray -d "" nccl_environment < <(env | grep -e "^NCCL_" | sed 's/^/-x /' | tr '\n' '\0')
          echo "Detected NCCL environment:"
          for nccl_variable in "${nccl_environment[@]}"; do
            echo "  $nccl_variable"
          done

          # export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
          # echo "Setting PYTORCH_CUDA_ALLOC_CONF=$PYTORCH_CUDA_ALLOC_CONF"

          worker_command="export RANK=\$OMPI_COMM_WORLD_RANK ;"
          worker_command+="export LOCAL_RANK=\$OMPI_COMM_WORLD_LOCAL_RANK ;"
          worker_command+="export HYDRA_FULL_ERROR=1 ;"
          worker_command+="echo \"Worker has started with rank \$RANK and local rank \$LOCAL_RANK\" ;"
          worker_command+="python $TORCH_DISTRIBUTED_TARGET --config-path=\"/etc/workload-configuration\" --config-name=\"nemo-configuration.yaml\" +trainer.num_nodes=\"$NNODES\" +exp_manager.version=\"$JOB_IDENTIFIER\" ${workload_arguments[@]}"
 
          echo "Launching MPI job across all hosts"
          export NVTE_UB_SOCKET_IFNAME="eth1"
          mpirun \
            --allow-run-as-root \
            -n "$((8*$NNODES))" \
            --mca orte_keep_fqdn_hostnames t \
            --mca plm_rsh_agent "ssh -q -o LogLevel=ERROR -o StrictHostKeyChecking=no -p 2222" \
            --hostfile /etc/job-worker-services.txt \
            --mca btl_tcp_if_include $NCCL_SOCKET_IFNAME \
            --mca btl tcp,self \
            -x LD_LIBRARY_PATH \
            -x MASTER_ADDR \
            -x MASTER_PORT \
            -x NNODES \
            -x GPUS_PER_NODE \
            -x GLOO_SOCKET_IFNAME \
            -x PYTORCH_CUDA_ALLOC_CONF \
            -x NVTE_FWD_LAYERNORM_SM_MARGIN \
            -x NVTE_BWD_LAYERNORM_SM_MARGIN \
            -x NVIDIA_PYTORCH_VERSION \
            -x NVTE_UB_SOCKET_IFNAME \
            ${nccl_environment[@]} \
            bash -c "$worker_command"

            # python $TORCH_DISTRIBUTED_TARGET \
            # --config-path="/etc/workload-configuration" \
            # --config-name="nemo-configuration.yaml" \
            # +trainer.num_nodes="$NNODES" \
            # +exp_manager.version="$JOB_IDENTIFIER" \
            # ${workload_arguments[@]} 
         
          echo "Informing all worker pods that the workload has terminated:"
          for JOB_INDEX in $(seq 0 $((NNODES-1))); do
            WORKER="$JOB_NAME-$JOB_INDEX.$JOB_NAME.default.svc.cluster.local"
            echo -n "  Write to $WORKER:/semaphore/workload_terminated to terminate worker: "
            ssh -o LogLevel=ERROR -o StrictHostKeyChecking=no -p 2222 $WORKER touch /semaphore/workload_terminated
            if [ "$?" -eq 0 ]; then
              echo "success"
            else
              echo "failed"
            fi
          done

        else 
          # We need the head node to tell worker when to exit
          while [ ! -e "/semaphore/workload_terminated" ]; do sleep 10; done
        fi


        # export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
        # echo "Setting PYTORCH_CUDA_ALLOC_CONF=$PYTORCH_CUDA_ALLOC_CONF"
        # OMP_NUM_THREADS=12 nsys profile -s none -t nvtx,cuda --capture-range=cudaProfilerApi --capture-range-end=stop \
        #   -o /gcs/nemo-experiments/$JOB_IDENTIFIER/rank-$NODE_RANK --force-overwrite true \
        #   --session-new nsys-$RANDOM \
        # torchrun \
        #   --nproc-per-node="${GPUS_PER_NODE}" \
        #   --nnodes="${NNODES}" \
        #   --node_rank="${NODE_RANK}" \
        #   --rdzv_id="${JOB_IDENTIFIER}" \
        #   --master_addr="${MASTER_ADDR}" \
        #   --master_port="${MASTER_PORT}" \
        # $TORCH_DISTRIBUTED_TARGET \
        #   --config-path="/etc/workload-configuration" \
        #   --config-name="nemo-configuration.yaml" \
        #   +trainer.num_nodes="$NNODES" \
        #   +exp_manager.version="$JOB_IDENTIFIER" \
        #   ${workload_arguments[@]} 
        #
        
        # for ((LOCAL_RANK=0; LOCAL_RANK <= $((GPUS_PER_NODE - 1)); LOCAL_RANK++)); do
        #   RANK=$((8*$NODE_RANK + $LOCAL_RANK))                                  
        #  
        #   OMP_NUM_THREADS=12 RANK=$RANK LOCAL_RANK=$LOCAL_RANK \
        #     nsys profile -s none -t nvtx,cuda --capture-range=cudaProfilerApi --capture-range-end=stop \
        #     -o /gcs/nemo-experiments/$JOB_IDENTIFIER/rank-$RANK \
        #     --session-new "nemo-rank$RANK-$RANDOM" \
        #     python $TORCH_DISTRIBUTED_TARGET \
        #     --config-path="/etc/workload-configuration" \
        #     --config-name="nemo-configuration.yaml" \
        #     +trainer.num_nodes="$NNODES" \
        #     +exp_manager.version="$JOB_IDENTIFIER" \
        #     ${workload_arguments[@]} &

        #   echo "Launched rank $RANK with PID $!"
        #   TORCH_PIDS[$LOCAL_RANK]=$!                                            
        # done  

        # Wait for Torch processes (might be problematic if only one fails)
        # for PID in ${TORCH_PIDS[*]}; do
        #   echo "Waiting on Torch PID $PID"
        #   wait $PID
        # done

        echo "Pod on $(hostname --fqdn) is exiting"
      volumeMounts:
        {{ if eq $root.Values.targetPlatform "gke" }}
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
        {{ else }}
        - name: dmabuf
          mountPath: /dev/dmabuf_import_helper
        - name: cuda-lib
          mountPath: /usr/lib/x86_64-linux-gnu/libcuda.so
        - name: cuda-lib1
          mountPath: /usr/lib/x86_64-linux-gnu/libcuda.so.1
        - name: cuda-lib535
          mountPath: /usr/lib/x86_64-linux-gnu/libcuda.so.535.104.12 
        {{ end }}      
        - name: gib
          mountPath: /usr/local/gib            
        - name: nccl-plugin-volume
          mountPath: /usr/local/nccl-plugin
        {{ if ne $root.Values.network.stack "tcp" }}
        - name: tcpx-daemon-socket
          mountPath: /tmp
        {{ end }}
        - name: workload-terminated-volume
          mountPath: /semaphore   
        - name: workload-configuration
          mountPath: /etc/workload-configuration  
        - name: shared-memory
          mountPath: /dev/shm 
        - name: local-ssd
          mountPath: "{{ $root.Values.volumes.ssdMountPath }}"

        {{- range $pvc := $root.Values.volumes.pvcMounts }}
        - name: "{{ $pvc.name }}"
          mountPath: "{{ $pvc.mountPath }}"
        {{- end }}

        {{- range $gcs := $root.Values.volumes.gcsMounts }}
        - name: "{{ $gcs.bucketName }}"
          mountPath: "{{ $gcs.mountPath }}"
        {{- end }}        

      resources:
        limits:
          nvidia.com/gpu: {{ $gpusPerNode }}
---